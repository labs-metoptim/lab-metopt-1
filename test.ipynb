{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.optimize import line_search\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "EPS = 1e-6\n",
    "MAX_EPOCH = 100000\n",
    "\n",
    "def distance(x, y):\n",
    "    return np.linalg.norm(x - y)\n",
    "\n",
    "class Function:\n",
    "    def __init__(self, function, grad, min_point=(0,0)):\n",
    "        self.call = function\n",
    "        self.grad = grad\n",
    "        self.min_point = min_point\n",
    "\n",
    "    def __call__(self, point):\n",
    "        return self.call(point)\n",
    "\n",
    "    def grad(self, point): return self.grad(point)\n",
    "\n",
    "def make_point(x, y): return np.array((x, y))\n",
    "\n",
    "def draw_function(f):\n",
    "    t = np.linspace(-5, 5, 100)\n",
    "    x, y = np.meshgrid(t, t)\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    ax.plot_surface(x, y, f((x, y)))\n",
    "\n",
    "def draw_gradient(f, points):\n",
    "    colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "    t = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(t, t)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(f((points[:, 0], points[:, 1])), lw=2, color=colors[0], )\n",
    "    ax1.grid()\n",
    "    \n",
    "    Z = f([X, Y])\n",
    "    cp = ax2.contourf(X, Y, Z)\n",
    "    fig.colorbar(cp)\n",
    "    ax2.plot(points[:, 0], points[:, 1], 'o-', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosy2(point):\n",
    "    x, y = point\n",
    "    return np.cos(x) + y**2\n",
    "\n",
    "def grad_cosy2(point):\n",
    "    x, y = point\n",
    "    return make_point(-np.sin(x), 2*y)\n",
    "\n",
    "fun = Function(cosy2, grad_cosy2, np.array((math.pi, 0)))\n",
    "draw_function(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def abstract_gradient_descent(first_point, next_point_fun, stop_criteria):\n",
    "    points = np.array([first_point])\n",
    "    x = first_point\n",
    "    epoch = 0\n",
    "    while not stop_criteria(x) and epoch < MAX_EPOCH:\n",
    "        x = next_point_fun(x, epoch)\n",
    "        points = np.append(points, [x], axis=0)\n",
    "        epoch += 1\n",
    "    return points\n",
    "\n",
    "def stop_by_grad(f):\n",
    "    return lambda x: np.linalg.norm(f.grad(x)) < EPS\n",
    "\n",
    "def lin_rate_grad_descent(f, first_point, lin_rate):\n",
    "    next_point = lambda x, epoch: x - lin_rate(epoch) * np.array(f.grad(x))\n",
    "    return abstract_gradient_descent(first_point, next_point, stop_by_grad(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, за сколько шагов наш градиентный спуск приходит к минимуму в зависимости от параметра linear_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(f, first_point, grad_descent, test_list):\n",
    "    def run(lr):\n",
    "        return grad_descent(f, first_point, lambda _: lr)\n",
    "    ans = pd.DataFrame()\n",
    "    ans[\"Linear rate\"] = test_list\n",
    "    ans[\"Epochs\"] = list(map(lambda a: a.size, map(run, test_list)))\n",
    "    return ans\n",
    "\n",
    "lin_rate_test_list = list(i for i in np.arange(0.4, 0.8, 0.02))\n",
    "test(fun, make_point(2, 1), lin_rate_grad_descent, lin_rate_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def exp_lin_rate_grad_descent(f, first_point, d):\n",
    "    return lin_rate_grad_descent(f, first_point, lambda epoch: np.exp(-d * epoch))\n",
    "\n",
    "def test(f, first_point, test_list):\n",
    "    def run(d):\n",
    "        return exp_lin_rate_grad_descent(f, first_point, d)\n",
    "    ans = pd.DataFrame()\n",
    "    ans[\"d\"] = test_list\n",
    "    ans[\"Epochs\"] = list(map(len, map(run, test_list)))\n",
    "    return ans\n",
    "\n",
    "exp_lin_rate_test_list = list(i for i in np.arange(0.05, 0.10, 0.005))\n",
    "test(fun, make_point(2, 1), exp_lin_rate_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Задание 3__\n",
    "Улучшим наш градиентный спуск с помощью дихотомии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dichotomy_grad_descent_constructor(left_bound, right_bound, delta):\n",
    "    def dichotomy(f):\n",
    "        left = left_bound\n",
    "        right = right_bound\n",
    "        x = left\n",
    "        while right - left >= EPS:\n",
    "            x = (left + right) / 2\n",
    "            if f(x - delta) <= f(x + delta):\n",
    "                right = x\n",
    "            else:\n",
    "                left = x\n",
    "        return x\n",
    "\n",
    "    def impl(f, first_point):\n",
    "        def next_point_fun(x, epoch):\n",
    "            d = dichotomy(lambda a: f(x - a * f.grad(x)))\n",
    "            return x - d * f.grad(x)\n",
    "        return abstract_gradient_descent(first_point, next_point_fun, stop_by_grad(f))\n",
    "    return impl\n",
    "\n",
    "dichotomy_grad_descent = dichotomy_grad_descent_constructor(-1, 1, 0.02)\n",
    "ps = dichotomy_grad_descent(fun, make_point(2, 1))\n",
    "draw_gradient(fun, ps)\n",
    "len(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4__\n",
    "Применим к полученному алгоритму усиленные условия Вольфе  \n",
    "  \n",
    "![alt text](Wolphe.png)    \n",
    "  \n",
    "Первое неравенство отвечает за уменьшение функции после совершения шага, второе неравенство -- за уменьшение проекции градиента по модулю, чтобы не допустить точек далеких от стационарных  \n",
    "Как правило : $c_1 = 0.0001$, $c_2 = 0.9$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dichotomy_grad_descent_constructor(left_bound, right_bound, delta):\n",
    "    def first_Wolphe_condition(alpha, x):\n",
    "        return np.all(fun(x - alpha*fun.grad(x)) <= fun(x) - C1 * alpha* fun.grad(x)**2)\n",
    "    def second_Wolphe_condition(alpha, x):\n",
    "        return np.all(abs(fun.grad(x - alpha * fun.grad(x)) * fun.grad(x)) <= C2 * abs(fun.grad(x)))\n",
    "    def Wolphe_conditions(alpha, x):\n",
    "        eps = 0\n",
    "        global C1\n",
    "        C1 = 1e-4\n",
    "        global C2\n",
    "        C2 = 0.9\n",
    "        for i in range(MAX_EPOCH):\n",
    "            alpha_right = alpha + eps\n",
    "            alpha_left = alpha - eps\n",
    "            if not (second_Wolphe_condition(alpha_right, x) and first_Wolphe_condition(alpha_right, x)):\n",
    "                if not (second_Wolphe_condition(alpha_left, x) and first_Wolphe_condition(alpha_left, x)):\n",
    "                    eps = eps + 0.001 if eps == 0 else eps * 2\n",
    "                else:\n",
    "                    return alpha_left\n",
    "            else:\n",
    "                return alpha_right            \n",
    "            \n",
    "    def dichotomy_with_Wolphe(f, cur_point):\n",
    "        left = left_bound\n",
    "        right = right_bound\n",
    "        alpha = left\n",
    "        while right - left >= EPS:\n",
    "            alpha = (left + right) / 2\n",
    "            if f(alpha - delta) <= f(alpha + delta):\n",
    "                right = alpha\n",
    "            else:\n",
    "                left = alpha\n",
    "\n",
    "        return Wolphe_conditions(alpha, cur_point)\n",
    "\n",
    "    def impl(f, first_point):\n",
    "        def next_point_fun(x, epoch):\n",
    "            d = dichotomy_with_Wolphe(lambda a: f(x - a * f.grad(x)), x)\n",
    "            return x - d * f.grad(x)\n",
    "        return abstract_gradient_descent(first_point, next_point_fun, stop_by_grad(f))\n",
    "    return impl\n",
    "\n",
    "dichotomy_grad_descent = dichotomy_grad_descent_constructor(-1, 1, 0.02)\n",
    "ps = dichotomy_grad_descent(fun, make_point(2, 1))\n",
    "draw_gradient(fun, ps)\n",
    "len(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func1(point):\n",
    "    x, y = point\n",
    "    return x**2 + y**2 - x * y\n",
    "\n",
    "def grad1(point):\n",
    "    x, y = point\n",
    "    return make_point(2*x - y, 2*y - x)\n",
    "\n",
    "fun1 = Function(func1, grad1, np.array((0, 0)))\n",
    "draw_function(fun1)\n",
    "\n",
    "def func2(point):\n",
    "    x, y = point\n",
    "    return x**2 - 2 * y**2\n",
    "\n",
    "def grad2(point):\n",
    "    x, y = point\n",
    "    return make_point(2*x, -4*y)\n",
    "\n",
    "fun2 = Function(func2, grad2, np.array((0, 0)))\n",
    "draw_function(fun2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_lin_rate = 0.65\n",
    "points_grad1 = lin_rate_grad_descent(fun1, make_point(4.5, 0), lambda _: common_lin_rate)\n",
    "points_grad2 = lin_rate_grad_descent(fun2, make_point(4.5, 0), lambda _: common_lin_rate)\n",
    "draw_gradient(fun1, points_grad1)\n",
    "draw_gradient(fun2, points_grad2)\n",
    "\n",
    "len(points_grad1)\n",
    "len(points_grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lin_rate_test_list = list(i for i in np.arange(0.1, 0.65, 0.05))\n",
    "\n",
    "test_epoch(fun1, make_point(4.5, 0), lin_rate_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef3c4596e5a56f64223cc09f7d55518e9f39e5df52ae77ce1687def7a2567576"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
